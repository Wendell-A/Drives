import os
import re
import logging
import pandas as pd
import requests
import time
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import List, Dict, Any

# ==============================================================================
# CONFIGURA√á√ÉO E LOGGING
# ==============================================================================
def setup_logging():
    """
    Configura o sistema de logging com:
    - Console: mostra todos os logs (INFO, WARNING, ERROR)
    - Arquivo: salva apenas WARNING e ERROR na pasta logs/
    """
    # Criar pasta de logs se n√£o existir
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    # Nome do arquivo de log baseado no nome do script
    script_name = Path(__file__).stem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = logs_dir / f"{script_name}_erros_{timestamp}.log"
    
    # Configurar formato dos logs
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # Handler para console (todos os n√≠veis)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # Handler para arquivo (apenas WARNING e ERROR)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.WARNING)  # Apenas WARNING e ERROR
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # Configurar o logger raiz
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.handlers.clear()  # Limpar handlers padr√£o
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)
    
    logging.info(f"üìù Sistema de logs configurado. Logs de erro ser√£o salvos em: {log_file}")
    return log_file

# Configurar logging
log_file_path = setup_logging()
load_dotenv()

class Config:
    TENANT_ID = os.getenv("TENANT_ID")
    CLIENT_ID = os.getenv("CLIENT_ID")
    CLIENT_SECRET = os.getenv("CLIENT_SECRET")
    HOSTNAME = os.getenv("HOSTNAME")
    
    SITE_PATH = "sites/Transportes"
    TARGET_SHEET_NAME = "Base"
    
    TRAFEGUS_FILENAME = "Relat√≥rio de NF Trafegus.xlsx"
    TRAFEGUS_SHEET_NAME = "Sheet1"
    
    # Nomes exatos das colunas no Relat√≥rio Trafegus
    COL_TRAFEGUS_PLACA = "Placa" # Ajuste se o nome exato for diferente (ex: "PLACA")
    COL_TRAFEGUS_POSICAO = "Posi√ß√£o" # Ajuste se o nome exato for diferente
    COL_TRAFEGUS_DATA_FIXA = "Data √öltima Posi√ß√£o" # Fixado conforme solicitado
    
    ARQUIVOS_PERMITIDOS = [
        "FORM-PPL-000 - Fitplan Hidratado - RJ.xlsx",
        "FORM-PPL-000 - Fitplan Hidratado - SP.xlsx",
        "FORM-PPL-000 - Fitplan Anidro - SP.xlsx",
        "FORM-PPL-000 - Fitplan Anidro - RJ.xlsx",
        'FORM-PPL-000 - Fitplan Biodiesel.xlsx',
        "FORM-PPL-000 - Gasolina.xlsx",
        "FORM-PPL-000 - Diesel e Insumos.xlsx"
    ]

    COLUNAS_TRANSPORTE = [
        "sm", "data_prev_carregamento", "expedidor", "cidade_origem", "ufo",
        "destinatario_venda", "destinatario", "recebedor", "cidade_destino", "ufd",
        "produto", "motorista", "cavalo", "carreta1", "carreta2", "transportadora",
        "nfe", "volume_l", "data_de_carregamento", 
        "horario_de_carregamento", 
        "data_chegada", "data_descarga", "status"
    ]

    @staticmethod
    def get_col_letter(col_name: str) -> str:
        try:
            idx = Config.COLUNAS_TRANSPORTE.index(col_name)
            return chr(65 + idx)
        except: return None

# ==============================================================================
# CLIENTE SHAREPOINT
# ==============================================================================
class SharePointClient:
    def __init__(self, config: Config):
        self.config = config
        self.access_token = self._get_token()
        self.api_site = f"{self.config.HOSTNAME}:/{self.config.SITE_PATH}"
        self.site_id = self._get_id('sites', self.api_site)
        self.drive_id = self._get_main_drive_id()

    def _get_token(self) -> str:
        url = f"https://login.microsoftonline.com/{self.config.TENANT_ID}/oauth2/v2.0/token"
        data = {
            "grant_type": "client_credentials",
            "client_id": self.config.CLIENT_ID,
            "client_secret": self.config.CLIENT_SECRET,
            "scope": "https://graph.microsoft.com/.default"
        }
        try:
            r = requests.post(url, data=data)
            r.raise_for_status()
            return r.json()["access_token"]
        except requests.exceptions.RequestException as e:
            status_code = getattr(e.response, 'status_code', None) if hasattr(e, 'response') else None
            response_text = getattr(e.response, 'text', None) if hasattr(e, 'response') and hasattr(e.response, 'text') else None
            logging.error(
                f"‚ùå ERRO ao obter token de autentica√ß√£o\n"
                f"   üîó URL: {url}\n"
                f"   üÜî Tenant ID: {self.config.TENANT_ID}\n"
                f"   üÜî Client ID: {self.config.CLIENT_ID}\n"
                f"   üìä Status Code: {status_code or 'N/A'}\n"
                f"   üìù Response: {response_text[:500] if response_text else 'N/A'}\n"
                f"   ‚ö†Ô∏è Erro: {type(e).__name__}: {str(e)}"
            )
            raise

    def _api_get(self, url: str) -> Any:
        headers = {"Authorization": f"Bearer {self.access_token}"}
        try:
            r = requests.get(url, headers=headers)
            r.raise_for_status()
            return r.json()
        except requests.exceptions.RequestException as e:
            status_code = getattr(e.response, 'status_code', None) if hasattr(e, 'response') else None
            response_text = getattr(e.response, 'text', None) if hasattr(e, 'response') and hasattr(e.response, 'text') else None
            logging.error(
                f"‚ùå ERRO na requisi√ß√£o GET\n"
                f"   üîó URL: {url}\n"
                f"   üìä Status Code: {status_code or 'N/A'}\n"
                f"   üìù Response: {response_text[:500] if response_text else 'N/A'}\n"
                f"   ‚ö†Ô∏è Erro: {type(e).__name__}: {str(e)}"
            )
            raise

    def _api_patch(self, url: str, json_data: Dict) -> Any:
        headers = {"Authorization": f"Bearer {self.access_token}", "Content-Type": "application/json"}
        try:
            r = requests.patch(url, headers=headers, json=json_data)
            r.raise_for_status()
            return r.json()
        except requests.exceptions.RequestException as e:
            status_code = getattr(e.response, 'status_code', None) if hasattr(e, 'response') else None
            response_text = getattr(e.response, 'text', None) if hasattr(e, 'response') and hasattr(e.response, 'text') else None
            logging.error(
                f"‚ùå ERRO na requisi√ß√£o PATCH\n"
                f"   üîó URL: {url}\n"
                f"   üì¶ Payload: {json_data}\n"
                f"   üìä Status Code: {status_code or 'N/A'}\n"
                f"   üìù Response: {response_text[:500] if response_text else 'N/A'}\n"
                f"   ‚ö†Ô∏è Erro: {type(e).__name__}: {str(e)}"
            )
            raise

    def _get_id(self, resource: str, path: str) -> str:
        return self._api_get(f"https://graph.microsoft.com/v1.0/{resource}/{path}")['id']

    def _get_main_drive_id(self) -> str:
        drives = self._api_get(f"https://graph.microsoft.com/v1.0/sites/{self.site_id}/drives")["value"]
        for d in drives:
            if d.get('name') == 'Documentos': return d['id']
        raise Exception("Biblioteca 'Documentos' n√£o encontrada.")

    def get_root_items(self) -> List[Dict]:
        return self._api_get(f"https://graph.microsoft.com/v1.0/drives/{self.drive_id}/root/children")["value"]

    def get_item_id_by_path(self, path: str) -> str:
        return self._api_get(f"https://graph.microsoft.com/v1.0/drives/{self.drive_id}/root:/{path}")['id']

    def read_excel(self, item_id: str, sheet_name: str, colunas_esperadas: List[str] = None) -> pd.DataFrame:
        try:
            sheets = self._api_get(f"https://graph.microsoft.com/v1.0/drives/{self.drive_id}/items/{item_id}/workbook/worksheets")["value"]
            actual_sheet = next((s['name'] for s in sheets if s['name'].lower() == sheet_name.lower()), sheets[0]['name'])
            url_range = f"https://graph.microsoft.com/v1.0/drives/{self.drive_id}/items/{item_id}/workbook/worksheets/{actual_sheet}/usedRange"
            data_json = self._api_get(url_range)
            values = data_json.get('values', [])
            if not values or len(values) < 2: 
                logging.warning(
                    f"‚ö†Ô∏è Excel vazio ou sem dados suficientes\n"
                    f"   üÜî Item ID: {item_id}\n"
                    f"   üìÑ Sheet esperado: '{sheet_name}' | Sheet usado: '{actual_sheet}'\n"
                    f"   üìä Linhas encontradas: {len(values) if values else 0}\n"
                    f"   üìã Colunas esperadas: {colunas_esperadas}"
                )
                return None
            df = pd.DataFrame(values[1:], columns=values[0])
            if colunas_esperadas:
                df = df.iloc[:, :len(colunas_esperadas)]
                df.columns = colunas_esperadas
            df['__ms_file_id'] = item_id
            df['__ms_sheet_name'] = actual_sheet
            df['__excel_row_num'] = range(2, len(df) + 2)
            return df
        except Exception as e:
            logging.error(
                f"‚ùå ERRO ao ler Excel\n"
                f"   üÜî Item ID: {item_id}\n"
                f"   üìÑ Sheet esperado: '{sheet_name}'\n"
                f"   üìã Colunas esperadas: {colunas_esperadas}\n"
                f"   ‚ö†Ô∏è Erro: {type(e).__name__}: {str(e)}"
            )
            return None

    def update_excel_row(self, item_id: str, sheet: str, row_num: int, updates: Dict[str, Any]):
        for col_name, value in updates.items():
            col_letter = Config.get_col_letter(col_name)
            if not col_letter: 
                logging.warning(f"‚ö†Ô∏è Coluna '{col_name}' n√£o encontrada no mapeamento. Colunas dispon√≠veis: {Config.COLUNAS_TRANSPORTE}")
                continue
            address = f"{col_letter}{row_num}"
            url = f"https://graph.microsoft.com/v1.0/drives/{self.drive_id}/items/{item_id}/workbook/worksheets/{sheet}/range(address='{address}')"
            payload = { "values": [[value]] }
            try:
                self._api_patch(url, payload)
                time.sleep(0.1) 
            except Exception as e:
                logging.error(
                    f"‚ùå ERRO ao atualizar c√©lula no Excel\n"
                    f"   üìç Localiza√ß√£o: Sheet='{sheet}' | C√©lula='{address}' | Linha={row_num}\n"
                    f"   üìù Coluna: '{col_name}' (letra: {col_letter})\n"
                    f"   üíæ Valor tentado: {repr(value)}\n"
                    f"   üîó URL: {url}\n"
                    f"   üì¶ Payload: {payload}\n"
                    f"   üÜî Item ID: {item_id}\n"
                    f"   ‚ö†Ô∏è Erro: {type(e).__name__}: {str(e)}"
                )

# ==============================================================================
# PROCESSADOR DE DADOS
# ==============================================================================
class DataProcessor:
    @staticmethod
    def normalizar(series: pd.Series) -> pd.Series:
        return series.astype(str).str.upper().str.strip()

    @staticmethod
    def limpar_placa(series: pd.Series) -> pd.Series:
        return series.astype(str).str.upper().str.replace(r'[^A-Z0-9]', '', regex=True)

    @staticmethod
    def limpar_data_com_extras(data_str: str) -> str:
        """
        Extrai apenas a parte da data (DD/MM/YYYY) de strings que cont√™m data + hora + dia da semana.
        
        Exemplos:
        - '09/02/2026 14:34:27 Seg' -> '09/02/2026'
        - '09/02/2026 14:34:27' -> '09/02/2026'
        - '09/02/2026 Seg' -> '09/02/2026'
        - '09/02/2026' -> '09/02/2026' (sem altera√ß√£o)
        
        Args:
            data_str: String que pode conter data com ou sem hora e dia da semana
            
        Returns:
            String com apenas a data no formato DD/MM/YYYY, ou string original se n√£o encontrar padr√£o
        """
        if not data_str or pd.isna(data_str):
            return ''
        
        data_str = str(data_str).strip()
        
        if not data_str or data_str.lower() == 'nan':
            return ''
        
        # Padr√£o regex para DD/MM/YYYY (com valida√ß√£o b√°sica)
        # Aceita: DD/MM/YYYY, D/MM/YYYY, DD/M/YYYY, D/M/YYYY
        pattern = r'^(\d{1,2}/\d{1,2}/\d{4})'
        match = re.match(pattern, data_str)
        
        if match:
            # Extrai apenas a parte da data
            data_limpa = match.group(1)
            return data_limpa
        else:
            # Se n√£o encontrar padr√£o, retorna string original
            return data_str

    @staticmethod
    def _tratar_data_excel(series: pd.Series, contexto: str = "") -> pd.Series:
        """
        Trata datas vindas do Excel que podem estar em diferentes formatos:
        - N√∫meros seriais do Excel (ex: 45322.0)
        - Strings em formato brasileiro (DD/MM/YYYY)
        - Strings em formato americano (MM/DD/YYYY)
        Retorna uma Series de datetime.
        
        Args:
            series: Series do pandas com valores de data
            contexto: Contexto adicional para os logs (ex: "Trafegus", "Transporte")
        """
        if series is None or series.empty:
            logging.debug(f"üìÖ [{contexto}] Series vazia ou None - retornando Series vazia")
            return pd.Series(dtype='datetime64[ns]')
        
        # Log estat√≠sticas do formato original
        total_valores = len(series)
        valores_nao_nulos = series.notna().sum()
        valores_nulos = total_valores - valores_nao_nulos
        
        # Analisar tipos dos valores n√£o nulos
        tipos_encontrados = {}
        formatos_encontrados = {}
        numericos_count = 0
        texto_count = 0
        
        for idx, val in series.items():
            if pd.notna(val):
                tipo = type(val).__name__
                tipos_encontrados[tipo] = tipos_encontrados.get(tipo, 0) + 1
                
                val_str = str(val).strip()
                formato = "desconhecido"
                
                # Verificar se √© num√©rico (serial do Excel)
                try:
                    num_val = float(val_str.replace(',', '.'))
                    if num_val > 0:
                        numericos_count += 1
                        formato = f"n√∫mero serial Excel ({num_val:.2f})"
                    else:
                        texto_count += 1
                        formato = "texto (n√∫mero <= 0)"
                except (ValueError, TypeError):
                    texto_count += 1
                    # Tentar identificar formato de texto
                    if '/' in val_str:
                        partes = val_str.split('/')
                        if len(partes) == 3:
                            primeiro = partes[0].strip()
                            segundo = partes[1].strip()
                            terceiro = partes[2].strip()
                            try:
                                p1 = int(primeiro)
                                p2 = int(segundo)
                                p3 = int(terceiro)
                                # Verificar se tem dados extras (hora, etc)
                                tem_extras = ' ' in val_str or len(terceiro) > 4
                                extras_info = " (com dados extras)" if tem_extras else ""
                                
                                # L√≥gica de detec√ß√£o: se primeiro <= 12 e segundo > 12, provavelmente MM/DD/YYYY
                                if p1 <= 12 and p2 > 12:
                                    formato = f"texto (MM/DD/YYYY?{extras_info})"
                                elif p1 > 12:
                                    formato = f"texto (DD/MM/YYYY?{extras_info})"
                                else:
                                    # Amb√≠guo (ex: 05/01/2024)
                                    formato = f"texto (amb√≠guo DD/MM ou MM/DD?{extras_info})"
                            except (ValueError, TypeError):
                                formato = "texto (formato com / mas n√£o num√©rico)"
                        else:
                            formato = "texto (formato com / mas n√£o 3 partes)"
                    elif '-' in val_str:
                        formato = "texto (formato com -)"
                    else:
                        formato = "texto (sem separador de data)"
                
                formatos_encontrados[formato] = formatos_encontrados.get(formato, 0) + 1
        
        logging.info(f"üìä [{contexto}] AN√ÅLISE DE DATAS - Total: {total_valores} | N√£o nulos: {valores_nao_nulos} | Nulos: {valores_nulos} | Num√©ricos: {numericos_count} | Texto: {texto_count}")
        if tipos_encontrados:
            logging.info(f"üìä [{contexto}] TIPOS ENCONTRADOS: {tipos_encontrados}")
        if formatos_encontrados:
            logging.info(f"üìä [{contexto}] FORMATOS DETECTADOS: {formatos_encontrados}")
        
        # ETAPA 0: Limpeza pr√©via - Remove hora e dia da semana das datas
        series_limpa = series.copy()
        datas_limpas_count = 0
        exemplos_limpeza = []
        
        for idx, val in series.items():
            if pd.notna(val):
                val_str = str(val).strip()
                val_limpo = DataProcessor.limpar_data_com_extras(val_str)
                if val_limpo != val_str:
                    series_limpa.iloc[idx] = val_limpo
                    datas_limpas_count += 1
                    if len(exemplos_limpeza) < 5:  # Guarda primeiros 5 exemplos
                        exemplos_limpeza.append((val_str, val_limpo))
        
        if datas_limpas_count > 0:
            logging.info(f"üßπ [{contexto}] {datas_limpas_count} datas foram limpas (remo√ß√£o de hora/dia da semana)")
            if exemplos_limpeza:
                logging.info(f"üßπ [{contexto}] Exemplos de limpeza (primeiros {len(exemplos_limpeza)}):")
                for antes, depois in exemplos_limpeza:
                    logging.info(f"   '{antes}' -> '{depois}'")
        
        # 1. Tenta converter valores num√©ricos do Excel (ex: 45322.0)
        # O Excel usa 1899-12-30 como origem para n√∫meros seriais de data
        datas_numericas = pd.to_numeric(series_limpa.astype(str).str.replace(',', '.'), errors='coerce')
        datas_convertidas = pd.to_datetime(datas_numericas, unit='D', origin='1899-12-30', errors='coerce')
        numericos_convertidos = datas_convertidas.notna().sum()
        
        if numericos_convertidos > 0:
            logging.info(f"‚úÖ [{contexto}] Convertidos {numericos_convertidos} valores num√©ricos (serial Excel)")
        
        # 2. Tenta converter texto com formato fixo brasileiro (DD/MM/YYYY)
        datas_texto = pd.to_datetime(series_limpa, format='%d/%m/%Y', errors='coerce')
        texto_convertido_fixo = datas_texto.notna().sum() - numericos_convertidos
        
        if texto_convertido_fixo > 0:
            logging.info(f"‚úÖ [{contexto}] Convertidos {texto_convertido_fixo} valores texto (formato DD/MM/YYYY fixo)")
        
        # 3. Se ainda houver NaT (falha no formato fixo), tenta o modo flex√≠vel com dayfirst=True
        # dayfirst=True for√ßa a interpreta√ß√£o brasileira (DD/MM/YYYY)
        mask_faltante = datas_texto.isna() & series_limpa.notna()
        if mask_faltante.any():
            valores_faltantes = mask_faltante.sum()
            logging.info(f"üîÑ [{contexto}] Tentando convers√£o flex√≠vel para {valores_faltantes} valores restantes...")
            datas_flexiveis = pd.to_datetime(series_limpa[mask_faltante], dayfirst=True, errors='coerce')
            flexiveis_convertidos = datas_flexiveis.notna().sum()
            if flexiveis_convertidos > 0:
                logging.info(f"‚úÖ [{contexto}] Convertidos {flexiveis_convertidos} valores com convers√£o flex√≠vel (dayfirst=True)")
            datas_texto = datas_texto.fillna(datas_flexiveis)
        
        # Verificar quantos valores n√£o foram convertidos
        resultado_final = datas_convertidas.fillna(datas_texto)
        nao_convertidos = resultado_final.isna().sum()
        if nao_convertidos > 0:
            # Log alguns exemplos dos valores que n√£o foram convertidos
            exemplos_nao_convertidos = series_limpa[resultado_final.isna()].head(10).tolist()
            indices_nao_convertidos = series_limpa[resultado_final.isna()].head(10).index.tolist()
            
            logging.warning(
                f"‚ö†Ô∏è [{contexto}] VALORES N√ÉO CONVERTIDOS PARA DATETIME\n"
                f"   üìä Total n√£o convertidos: {nao_convertidos} de {total_valores}\n"
                f"   üìù Exemplos (primeiros 10):\n"
                + "\n".join([f"      [{idx}] {repr(val)}" for idx, val in zip(indices_nao_convertidos, exemplos_nao_convertidos)])
            )
        
        # Combina os resultados: prioriza datas num√©ricas, depois texto
        return resultado_final

    @staticmethod
    def formatar_data_brasileira(data_value, contexto: str = "") -> str:
        """
        Converte um valor de data para string no formato brasileiro (DD/MM/YYYY).
        Aceita: datetime, string, n√∫mero serial do Excel, ou None.
        
        Args:
            data_value: Valor da data a ser formatado
            contexto: Contexto adicional para os logs (ex: "Trafegus", "Transporte")
        """
        # Identificar tipo e formato original
        tipo_original = type(data_value).__name__
        valor_original_str = str(data_value).strip()
        
        if pd.isna(data_value) or data_value is None or valor_original_str == '' or valor_original_str.lower() == 'nan':
            logging.debug(f"üìÖ [{contexto}] Data vazia ou nula - retornando vazio")
            return ''
        
        try:
            # Se j√° for datetime, formata diretamente
            if isinstance(data_value, (pd.Timestamp, datetime)):
                formato_final = data_value.strftime('%d/%m/%Y')
                logging.info(f"üìÖ [{contexto}] FORMATO LOCALIZADO: datetime | VALOR: {data_value} | FORMATO REPASSADO: {formato_final} | TIPO: datetime")
                return formato_final
            
            # Se for string, limpa primeiro (remove hora e dia da semana)
            data_str_original = valor_original_str
            data_str = DataProcessor.limpar_data_com_extras(valor_original_str)
            
            # Verificar se houve limpeza (dados extras removidos)
            tem_dados_extras = False
            dados_extras_info = ""
            if data_str != data_str_original:
                dados_extras_info = f" | DADOS EXTRAS REMOVIDOS: '{data_str_original[len(data_str):].strip()}'"
                tem_dados_extras = True
            
            # Verificar se √© n√∫mero (serial do Excel)
            is_numero = False
            try:
                num_val = float(data_str.replace(',', '.'))
                if num_val > 0:
                    is_numero = True
                    dt = pd.to_datetime(num_val, unit='D', origin='1899-12-30')
                    formato_final = dt.strftime('%d/%m/%Y')
                    logging.info(f"üìÖ [{contexto}] FORMATO LOCALIZADO: n√∫mero serial Excel ({num_val}) | VALOR ORIGINAL: {valor_original_str}{dados_extras_info} | FORMATO REPASSADO: {formato_final} | TIPO: n√∫mero")
                    return formato_final
            except (ValueError, TypeError):
                pass
            
            # Tenta converter string de data - Primeiro formato brasileiro fixo (DD/MM/YYYY)
            try:
                dt = pd.to_datetime(data_str, format='%d/%m/%Y')
                formato_final = dt.strftime('%d/%m/%Y')
                tipo_detectado = "texto (DD/MM/YYYY)"
                logging.info(f"üìÖ [{contexto}] FORMATO LOCALIZADO: {tipo_detectado} | VALOR ORIGINAL: {valor_original_str}{dados_extras_info} | FORMATO REPASSADO: {formato_final} | TIPO: texto")
                return formato_final
            except (ValueError, TypeError):
                pass
            
            # Tenta formato americano ou amb√≠guo com dayfirst=True (for√ßa interpreta√ß√£o brasileira)
            try:
                dt = pd.to_datetime(data_str, dayfirst=True, errors='coerce')
                if pd.notna(dt):
                    formato_final = dt.strftime('%d/%m/%Y')
                    # Tentar detectar se era formato americano
                    if '/' in data_str:
                        partes = data_str.split('/')
                        if len(partes) == 3:
                            primeiro = partes[0]
                            segundo = partes[1]
                            # Se primeiro > 12, provavelmente era DD/MM/YYYY
                            # Se primeiro <= 12 e segundo > 12, provavelmente era MM/DD/YYYY
                            if int(primeiro) <= 12 and int(segundo) > 12:
                                tipo_detectado = "texto (MM/DD/YYYY - CORRIGIDO)"
                            else:
                                tipo_detectado = "texto (DD/MM/YYYY - confirmado)"
                        else:
                            tipo_detectado = "texto (formato flex√≠vel)"
                    else:
                        tipo_detectado = "texto (formato flex√≠vel)"
                    
                    logging.info(f"üìÖ [{contexto}] FORMATO LOCALIZADO: {tipo_detectado} | VALOR ORIGINAL: {valor_original_str}{dados_extras_info} | FORMATO REPASSADO: {formato_final} | TIPO: texto")
                    return formato_final
            except (ValueError, TypeError) as e:
                pass
            
            # Se tudo falhar, retorna a string original (pode ser um formato n√£o reconhecido)
            logging.warning(
                f"‚ö†Ô∏è [{contexto}] FALHA NA CONVERS√ÉO DE DATA\n"
                f"   üìÖ FORMATO LOCALIZADO: DESCONHECIDO\n"
                f"   üìù VALOR ORIGINAL: {repr(valor_original_str)}{dados_extras_info}\n"
                f"   üî¢ TIPO ORIGINAL: {tipo_original}\n"
                f"   üì§ FORMATO REPASSADO: {valor_original_str} (sem convers√£o)\n"
                f"   ‚ö†Ô∏è Motivo: Nenhum m√©todo de convers√£o funcionou"
            )
            return valor_original_str
            
        except Exception as e:
            logging.error(
                f"‚ùå [{contexto}] ERRO CR√çTICO ao formatar data\n"
                f"   üìù VALOR ORIGINAL: {repr(valor_original_str)}\n"
                f"   üî¢ TIPO ORIGINAL: {tipo_original}\n"
                f"   ‚ö†Ô∏è Erro: {type(e).__name__}: {str(e)}\n"
                f"   üì§ Retornando valor como string: {str(data_value)}"
            )
            return str(data_value)

    @staticmethod
    def formatar_string_final(row):
        # Trata a data corretamente antes de formatar
        data_origem_raw = row[Config.COL_TRAFEGUS_DATA_FIXA]
        arquivo_nome = row.get('__arquivo_nome', 'Desconhecido')
        cavalo = row.get('cavalo', 'N/A')
        linha_excel = row.get('__excel_row_num', 'N/A')
        contexto = f"Arquivo: {arquivo_nome} | Placa: {cavalo} | Linha: {linha_excel}"
        data_origem = DataProcessor.formatar_data_brasileira(data_origem_raw, contexto=contexto)
        
        posicao_original = str(row['ultima_posicao_original']).strip()
        posicao_norm = str(row['ultima_posicao_norm'])
        status_atual = str(row['status_norm'])

        # L√≥gica Condicional de Verifica√ß√£o
        no_local = False
        
        if status_atual == 'PROGRAMADO':
            # Para Programados, olha a ORIGEM
            expedidor = str(row['expedidor_norm'])
            cidade_origem = str(row['cidade_origem_norm'])
            if (expedidor != "" and expedidor in posicao_norm) or \
               (cidade_origem != "" and cidade_origem in posicao_norm):
                no_local = True
        
        elif 'TRANSITO' in status_atual:
            # Para Em Tr√¢nsito, olha o DESTINO
            cidade_destino = str(row['cidade_destino_norm'])
            if cidade_destino != "" and cidade_destino in posicao_norm:
                no_local = True

        # Se estiver "NO LOCAL", n√£o precisa de data - √© um resultado v√°lido
        if no_local:
            # Se a data estiver vazia mas est√° "NO LOCAL", n√£o √© erro - apenas log informativo
            if not data_origem or data_origem.strip() == '':
                logging.info(
                    f"‚ÑπÔ∏è [{contexto}] Ve√≠culo NO LOCAL (sem data do Trafegus)\n"
                    f"   üìÑ Arquivo: {arquivo_nome}\n"
                    f"   üöõ Placa: {cavalo}\n"
                    f"   üìç Linha Excel: {linha_excel}\n"
                    f"   üìù Valor original Trafegus: {repr(data_origem_raw)}\n"
                    f"   ‚úÖ Resultado: ' | NO LOCAL' (v√°lido - ve√≠culo j√° no local)"
                )
                return " | NO LOCAL"
            else:
                return f"{data_origem} | NO LOCAL"
        else:
            # Se N√ÉO estiver "NO LOCAL" e a data estiver vazia, a√≠ sim √© um erro
            if not data_origem or data_origem.strip() == '':
                logging.error(
                    f"‚ùå [{contexto}] DATA VAZIA AP√ìS FORMATA√á√ÉO (ve√≠culo n√£o est√° no local)\n"
                    f"   üìù Valor original: {repr(data_origem_raw)}\n"
                    f"   üìÑ Arquivo: {arquivo_nome}\n"
                    f"   üöõ Placa: {cavalo}\n"
                    f"   üìç Linha Excel: {linha_excel}\n"
                    f"   üìç Posi√ß√£o: {posicao_original}\n"
                    f"   üîÑ Usando fallback: data atual"
                )
                # Usar data atual como fallback apenas quando n√£o est√° "NO LOCAL"
                data_origem = datetime.now().strftime('%d/%m/%Y')
            
            return f"{data_origem} | {posicao_original}"

# ==============================================================================
# EXECU√á√ÉO PRINCIPAL
# ==============================================================================
def main():
    try:
        sp = SharePointClient(Config)

        logging.info("üìÇ Lendo arquivos de transporte...")
        arquivos = sp.get_root_items()
        lista_dfs = []

        for arq in arquivos:
            if arq['name'] in Config.ARQUIVOS_PERMITIDOS:
                # LOG DE ARQUIVO LIDO
                logging.info(f"   [CHECK] Processando arquivo: {arq['name']}")
                
                df = sp.read_excel(arq['id'], Config.TARGET_SHEET_NAME, Config.COLUNAS_TRANSPORTE)
                if df is not None:
                    df['__arquivo_nome'] = arq['name']
                    lista_dfs.append(df)

        if not lista_dfs:
            arquivos_encontrados = [arq['name'] for arq in arquivos]
            logging.warning(
                f"‚ö†Ô∏è NENHUM ARQUIVO DE TRANSPORTE PERMITIDO ENCONTRADO\n"
                f"   üìã Arquivos permitidos: {Config.ARQUIVOS_PERMITIDOS}\n"
                f"   üìÇ Arquivos encontrados no root ({len(arquivos_encontrados)}):\n"
                + "\n".join([f"      - {nome}" for nome in arquivos_encontrados[:20]])  # Limita a 20 para n√£o poluir
            )
            return

        df_transp = pd.concat(lista_dfs, ignore_index=True)
        df_transp['status_norm'] = DataProcessor.normalizar(df_transp['status'])
        
        # Filtra Programados e Em Tr√¢nsito
        status_permitidos = ['PROGRAMADO', 'EM TR√ÇNSITO', 'EM TR√ÇNSITO BY PASS']
        df_transp = df_transp[df_transp['status_norm'].isin(status_permitidos)].copy()

        if df_transp.empty:
            logging.info("üí§ Nenhuma viagem nos status permitidos para processar.")
            return

        # Normaliza√ß√µes para o "Match" de localiza√ß√£o
        df_transp['cavalo_match'] = DataProcessor.limpar_placa(df_transp['cavalo'])
        df_transp['expedidor_norm'] = DataProcessor.normalizar(df_transp['expedidor'])
        df_transp['cidade_origem_norm'] = DataProcessor.normalizar(df_transp['cidade_origem'])
        df_transp['cidade_destino_norm'] = DataProcessor.normalizar(df_transp['cidade_destino'])

        # LER TRAFEGUS
        logging.info(f"üìÑ Lendo fonte de dados: {Config.TRAFEGUS_FILENAME}")
        trafegus_id = sp.get_item_id_by_path(Config.TRAFEGUS_FILENAME)
        df_trafegus = sp.read_excel(trafegus_id, Config.TRAFEGUS_SHEET_NAME)
        
        # Valida√ß√£o das colunas fixas
        if Config.COL_TRAFEGUS_DATA_FIXA not in df_trafegus.columns:
            colunas_encontradas = list(df_trafegus.columns)
            logging.error(
                f"‚ùå COLUNA OBRIGAT√ìRIA N√ÉO ENCONTRADA NO TRAFEGUS\n"
                f"   üîç Coluna esperada: '{Config.COL_TRAFEGUS_DATA_FIXA}'\n"
                f"   üìã Colunas encontradas ({len(colunas_encontradas)}):\n"
                + "\n".join([f"      - {col}" for col in colunas_encontradas[:30]])  # Limita a 30
            )
            return

        # Identificar colunas de Placa e Posi√ß√£o (caso variem, mas mantendo a l√≥gica)
        col_placa = next((c for c in df_trafegus.columns if 'PLACA' in c.upper()), None)
        col_posicao = next((c for c in df_trafegus.columns if 'POSI' in c.upper() or 'LOCALIZA' in c.upper()), None)
        
        if not col_placa:
            colunas_encontradas = list(df_trafegus.columns)
            logging.error(
                f"‚ùå COLUNA DE PLACA N√ÉO ENCONTRADA NO TRAFEGUS\n"
                f"   üîç Buscando coluna contendo 'PLACA'\n"
                f"   üìã Colunas dispon√≠veis ({len(colunas_encontradas)}):\n"
                + "\n".join([f"      - {col}" for col in colunas_encontradas[:30]])
            )
            return
        
        if not col_posicao:
            colunas_encontradas = list(df_trafegus.columns)
            logging.error(
                f"‚ùå COLUNA DE POSI√á√ÉO N√ÉO ENCONTRADA NO TRAFEGUS\n"
                f"   üîç Buscando coluna contendo 'POSI' ou 'LOCALIZA'\n"
                f"   üìã Colunas dispon√≠veis ({len(colunas_encontradas)}):\n"
                + "\n".join([f"      - {col}" for col in colunas_encontradas[:30]])
            )
            return

        # Tratamento correto da coluna de data do Trafegus
        logging.info("üîß Tratando coluna de data do Trafegus...")
        df_trafegus[Config.COL_TRAFEGUS_DATA_FIXA] = DataProcessor._tratar_data_excel(
            df_trafegus[Config.COL_TRAFEGUS_DATA_FIXA],
            contexto="Trafegus"
        )

        df_trafegus['placa_match'] = DataProcessor.limpar_placa(df_trafegus[col_placa])
        df_trafegus['ultima_posicao_original'] = df_trafegus[col_posicao].astype(str)
        df_trafegus['ultima_posicao_norm'] = DataProcessor.normalizar(df_trafegus[col_posicao])

        # Merge
        df_match = pd.merge(
            df_transp, 
            df_trafegus[['placa_match', Config.COL_TRAFEGUS_DATA_FIXA, 'ultima_posicao_norm', 'ultima_posicao_original']], 
            left_on='cavalo_match', 
            right_on='placa_match', 
            how='inner'
        )

        for _, row in df_match.iterrows():
            nova_info = DataProcessor.formatar_string_final(row)
            updates = { "data_chegada": nova_info }
            
            logging.info(f"üíæ Atualizando {row['__arquivo_nome']} | Linha {row['__excel_row_num']} | {row['cavalo']} -> {nova_info}")
            
            sp.update_excel_row(
                row['__ms_file_id'], 
                row['__ms_sheet_name'], 
                row['__excel_row_num'], 
                updates
            )

        logging.info("‚úÖ Sincroniza√ß√£o Trafegus finalizada.")

    except Exception as e:
        import traceback
        logging.critical(
            f"üî• ERRO FATAL NA EXECU√á√ÉO\n"
            f"   ‚ö†Ô∏è Tipo do erro: {type(e).__name__}\n"
            f"   üìù Mensagem: {str(e)}\n"
            f"   üìç Traceback completo:\n"
            + "\n".join([f"      {linha}" for linha in traceback.format_exc().split('\n') if linha.strip()])
        )

if __name__ == "__main__":
    main()